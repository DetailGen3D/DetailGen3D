<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>DetailGen3D: Generative 3D Geometry Enhancement via Data-Dependent Flow</title>
	<!-- <link rel="icon" type="image/x-icon" href="../assets/css/images/favicon.ico"> -->
    <meta content="DetailGen3D: Generative 3D Geometry Enhancement via Data-Dependent Flow" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <!-- model viewer -->
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MLDP9MKGC8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MLDP9MKGC8');
    </script>
</head>

<body>
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">DetailGen3D: Generative 3D Geometry Enhancement via Data-Dependent Flow</h1>
            <div class="nerf_subheader_v2">Arxiv 2024</div>
            <div class="nerf_subheader_v2">
                <div style="text-align: center; margin-bottom: 20px;">
                    <!-- 作者部分 -->
                    <div style="margin-bottom: 15px;">
                        <a href="" target="_blank" class="nerf_authors_v2" style="margin-right: 15px; position: relative;">
                            Ken Deng<sup style="position: relative; top: -0.3em; left: -0em;">1,2,3</sup>
                        </a>
                        <a href="" target="_blank" class="nerf_authors_v2" style="margin-right: 15px; position: relative;">
                            Yuanchen Guo<sup style="position: relative; top: -0.3em; left: -0em;">2</sup>
                        </a>
                        <a href="" target="_blank" class="nerf_authors_v2" style="margin-right: 15px; position: relative;">
                            Jingxiang Sun<sup style="position: relative; top: -0.3em; left: -0em;">1</sup>
                        </a>
                        <a href="" target="_blank" class="nerf_authors_v2" style="margin-right: 15px; position: relative;">
                            Zixin Zou<sup style="position: relative; top: -0.3em; left: -0em;">2</sup>
                        </a>
                        <a href="" target="_blank" class="nerf_authors_v2" style="margin-right: 15px; position: relative;">
                            Yangguang Li<sup style="position: relative; top: -0.3em; left: -0em;">2</sup>
                        </a>
                        <a href="" target="_blank" class="nerf_authors_v2" style="margin-right: 15px; position: relative;">
                            Xin Cai<sup style="position: relative; top: -0.3em; left: -0em;">4</sup>
                        </a>
                        <br>
                        <a href="" target="_blank" class="nerf_authors_v2" style="margin-right: 15px; position: relative;">
                            Yanpei Cao<sup style="position: relative; top: -0.3em; left: -0em;">2</sup>
                        </a>
                        <a href="" target="_blank" class="nerf_authors_v2" style="margin-right: 15px; position: relative;">
                            Yebin Liu<sup style="position: relative; top: -0.3em; left: -0em;">1</sup>
                        </a>
                        <a href="" target="_blank" class="nerf_authors_v2" style="margin-right: 15px; position: relative;">
                            Ding Liang<sup style="position: relative; top: -0.3em; left: -0em;">1,2</sup>
                        </a>
                    </div>
                
                    <!-- 单位部分 -->
                    <div style="font-size: 14px; line-height: 1.8;">
                        <span class="nerf_affiliation_v2">
                            <sup>1</sup> Tsinghua University, 
                        </span>
                        <span class="nerf_affiliation_v2">
                            <sup>2</sup> VAST, 
                        </span>
                        <span class="nerf_affiliation_v2">
                            <sup>3</sup> Sun Yat-sen University, 
                        </span>
                        <span class="nerf_affiliation_v2">
                            <sup>4</sup> The Chinese University of Hong Kong
                        </span>
                    </div>
                    <div class="external-link">
                        <a class="btn" href="" role="button" target="_blank">
                            <i class="ai ai-arxiv"></i> Arxiv </a>
                        <a class="btn" href="" role="button" target="_blank">
                            <i class="fa fa-file-pdf"></i> Paper </a>
                            <a class="btn" href="index.html" role="button" disabled>
                                <i class="fa fa-home"></i> Home </a>
                            <a class="btn" href="" role="button" target="_blank" disabled>
                                <i class="fa-brands fa-github"></i> Code </a>
                        <a class="btn btn-large btn-light" href="" role="button" target="_blank" disabled>
                            <i class="fa-brands fa-youtube"></i> Video </a>
                    </div>
                </div>
                

            </div>
        </div>

    </div>


    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                Modern 3D generation methods can rapidly create shapes from sparse or single views, but their outputs often lack geometric detail due to computational constraints.  We present DetailGen3D, a generative approach specifically designed to enhance these generated 3D shapes. Our key insight is to model the coarse-to-fine transformation directly through data-dependent flows in latent space, avoiding the computational overhead of large-scale 3D generative models. We introduce a token matching strategy that ensures accurate spatial correspondence during refinement, enabling local detail synthesis while preserving global structure. By carefully designing our training data to match the characteristics of synthesized coarse shapes, our method can effectively enhance shapes produced by various 3D generation and reconstruction approaches, from single-view to sparse multi-view inputs. Extensive experiments demonstrate that DetailGen3D achieves high-fidelity geometric detail synthesis while maintaining efficiency in training.
                <br>
                <!-- <img  src="assets/images/overview.png"> -->
            </p>
        </div>
    </div>


    <div class="white_section_nerf  w-container">
        <div class="white_section_nerf  w-container">
            <h2 class="grey-heading_nerf">Method Overview</h2>
            <div class="grid-container-1">
                <img src="assets/images/overview_pipelines.png">
    
                <p>(1) Inference pipeline. We use FPS-VAE to extract tokens of the coarse geometry generated or reconstructed, then input the coarse token and DINO feature of the image prompt to DiT. After denoising, we decode the predicted token using FPS-VAE decoder to obtain refined geometry. The inference process takes only a few seconds. (2) For training data, we use reconstruction results reconstructed by LRM using multi-views rendered from fine geometry as coarse geometry. (3) We demonstrate the token matching process on the left. On the right, for the top one, we only use part query points, which are located in quadrant one, and for the bottom one, we use full query points, which demonstrate tokens represent the space around the corresponding query points. 
                </p>
            </div>
        </div>
        <h2 class="grey-heading_nerf">Refine Results</h2>
        <div class="grid-container-4">
            <div>
                <p class="myprompt nerf_text">Image prompt<br>&nbsp</p>
                <img class="image" id="image1" src="assets/input_images/2e2359e8-6aa1-46b0-9ca4-2f835b230544.webp" alt="Sample Image">
            </div>
            <div>
                <p class="myprompt nerf_text">Normal <br>&nbsp</p>
                <video class="video" id="1" loop playsinline autoPlay muted
                src="assets/teaser/videos/2e2359e8-6aa1-46b0-9ca4-2f835b230544.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="1_merge"></canvas>
            </div>
            <div class="video">
                <p class="myprompt nerf_text"  style="margin: 0; padding: 0;">Coarse mesh</p>
                <model-viewer src="assets/teaser/coarse_mesh/2e2359e8-6aa1-46b0-9ca4-2f835b230544.glb" style="width: 100%; height: 180px; margin-top: 30px;" auto-rotate shadow-intensity="1" camera-orbit = "-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>
            <div class="video">
                <p class="myprompt nerf_text" style="margin: 0; padding: 0;">Fine mesh</p>
                <model-viewer src="assets/teaser/fine_mesh/2e2359e8-6aa1-46b0-9ca4-2f835b230544.glb" style="width: 100%; height: 180px; margin-top: 30px;" auto-rotate shadow-intensity="1" camera-orbit="-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>
            


            <div>
                <!-- <p class="myprompt nerf_text">Image prompt<br>&nbsp</p> -->
                <img class="image" id="image1" src="assets/input_images/25f7fcac-367f-4078-9082-75e79a52dbf5.webp" alt="Sample Image">
            </div>
            <div>
                <!-- <p class="myprompt nerf_text">Normal <br>&nbsp</p> -->
                <video class="video" id="2" loop playsinline autoPlay muted
                src="assets/teaser/videos/25f7fcac-367f-4078-9082-75e79a52dbf5.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="2_merge"></canvas>
            </div>
            <div class="video">
                <!-- <p class="myprompt nerf_text"  style="margin: 0; padding: 0;">Coarse mesh</p> -->
                <model-viewer src="assets/teaser/coarse_mesh/25f7fcac-367f-4078-9082-75e79a52dbf5.glb" style="width: 100%; height: 180px; margin-top: 0px;" auto-rotate shadow-intensity="1" camera-orbit = "-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>
            <div class="video">
                <!-- <p class="myprompt nerf_text" style="margin: 0; padding: 0;">Fine mesh</p> -->
                <model-viewer src="assets/teaser/fine_mesh/25f7fcac-367f-4078-9082-75e79a52dbf5.glb" style="width: 100%; height: 180px; margin-top: 0px;" auto-rotate shadow-intensity="1" camera-orbit="-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>


            <div>
                <!-- <p class="myprompt nerf_text">Image prompt<br>&nbsp</p> -->
                <img class="image" id="image1" src="assets/input_images/9918542e-e6c3-4a9b-bb78-554d77d7e2cf.webp" alt="Sample Image">
            </div>
            <div>
                <!-- <p class="myprompt nerf_text">Normal <br>&nbsp</p> -->
                <video class="video" id="3" loop playsinline autoPlay muted
                src="assets/teaser/videos/9918542e-e6c3-4a9b-bb78-554d77d7e2cf.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="3_merge"></canvas>
            </div>
            <div class="video">
                <!-- <p class="myprompt nerf_text"  style="margin: 0; padding: 0;">Coarse mesh</p> -->
                <model-viewer src="assets/teaser/coarse_mesh/9918542e-e6c3-4a9b-bb78-554d77d7e2cf.glb" style="width: 100%; height: 180px; margin-top: 0px;" auto-rotate shadow-intensity="1" camera-orbit = "-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>
            <div class="video">
                <!-- <p class="myprompt nerf_text" style="margin: 0; padding: 0;">Fine mesh</p> -->
                <model-viewer src="assets/teaser/fine_mesh/9918542e-e6c3-4a9b-bb78-554d77d7e2cf.glb" style="width: 100%; height: 180px; margin-top: 0px;" auto-rotate shadow-intensity="1" camera-orbit="-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>

            <div>
                <!-- <p class="myprompt nerf_text">Image prompt<br>&nbsp</p> -->
                <img class="image" id="image1" src="assets/input_images/cb7e6c4a-b4dd-483c-9789-3d4887ee7434.webp" alt="Sample Image">
            </div>
            <div>
                <!-- <p class="myprompt nerf_text">Normal <br>&nbsp</p> -->
                <video class="video" id="4" loop playsinline autoPlay muted
                src="assets/experiment/exp_video/generate/lyg_my_val/CRM/cb7e6c4a-b4dd-483c-9789-3d4887ee7434.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="4_merge"></canvas>
            </div>
            <div class="video">
                <!-- <p class="myprompt nerf_text"  style="margin: 0; padding: 0;">Coarse mesh</p> -->
                <model-viewer src="assets/experiment/mesh/generate/lyg_my_val/CRM/coarse/cb7e6c4a-b4dd-483c-9789-3d4887ee7434.glb" style="width: 100%; height: 180px; margin-top: 0px;" auto-rotate shadow-intensity="1" camera-orbit = "-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>
            <div class="video">
                <!-- <p class="myprompt nerf_text" style="margin: 0; padding: 0;">Fine mesh</p> -->
                <model-viewer src="assets/experiment/mesh/generate/lyg_my_val/CRM/fine/cb7e6c4a-b4dd-483c-9789-3d4887ee7434.glb" style="width: 100%; height: 180px; margin-top: 0px;" auto-rotate shadow-intensity="1" camera-orbit="-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>

        
            <div>
                <!-- <p class="myprompt nerf_text">Image prompt<br>&nbsp</p> -->
                <img class="image" id="image1" src="assets/input_images/b573ba71-409e-453d-8ef6-e5f4c5f86628.webp" alt="Sample Image">
            </div>
            <div>
                <!-- <p class="myprompt nerf_text">Normal <br>&nbsp</p> -->
                <video class="video" id="5" loop playsinline autoPlay muted
                src="assets/experiment/exp_video/generate/lyg_my_val/instantmesh_nerf/b573ba71-409e-453d-8ef6-e5f4c5f86628.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="5_merge"></canvas>
            </div>
            <div class="video">
                <!-- <p class="myprompt nerf_text"  style="margin: 0; padding: 0;">Coarse mesh</p> -->
                <model-viewer src="assets/experiment/mesh/generate/lyg_my_val/instantmesh_nerf/coarse/b573ba71-409e-453d-8ef6-e5f4c5f86628.glb" style="width: 100%; height: 180px; margin-top: 0px;" auto-rotate shadow-intensity="1" camera-orbit = "-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>
            <div class="video">
                <!-- <p class="myprompt nerf_text" style="margin: 0; padding: 0;">Fine mesh</p> -->
                <model-viewer src="assets/experiment/mesh/generate/lyg_my_val/instantmesh_nerf/fine/b573ba71-409e-453d-8ef6-e5f4c5f86628.glb" style="width: 100%; height: 180px; margin-top: 0px;" auto-rotate shadow-intensity="1" camera-orbit="-45deg 90deg 100%" camera-controls touch-action="pan-y"></model-viewer>
            </div>


            

        </div>
        <div class="grid-container-1">
            <a class="mybtn" href="more_results.html" role="button">
             More Results </a>
        </div>
        
    </div>
<!-- 
    <div class="white_section_nerf  w-container">
        <h2 class="grey-heading_nerf">Ablation Study</h2>
        <div>
            <video class="video" loop playsinline autoPlay muted
            src="assets/ablation_study/ablation_study.mp4" onplay="resizeAndPlay(this)"></video>
            <canvas class="video"></canvas>
        </div>
    </div> -->



<!-- <div class="white_section_nerf grey_container w-container">
<h2 class="grey-heading_nerf">BibTeX</h2>
<div class="bibtex">
    <pre><code>@article{sweetdreamer,
author    = {Weiyu Li and Rui Chen and Xuelin Chen and Ping Tan},
title     = {SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D},
journal   = {arxiv:2310.02596},
year      = {2023},
}</code></pre>
</div>
</div> -->

</body>
<footer>
    This project page is inspired by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies.</a>, © Weiyu Li. All rights reserved.
</footer>

</html>
